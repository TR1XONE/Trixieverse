/**\n * Moderation Service\n * Toxicity detection and content moderation\n */\n\nimport db from '../database/connection.js';\nimport logger from '../utils/logger.js';\nimport { OpenAI } from 'openai';\n\nconst openai = new OpenAI({\n  apiKey: process.env.OPENAI_API_KEY,\n});\n\ninterface ModerationResult {\n  text: string;\n  isToxic: boolean;\n  toxicityScore: number;\n  categories: string[];\n  action: 'allow' | 'warn' | 'mute' | 'ban';\n}\n\nclass ModerationService {\n  private toxicKeywords = [\n    'noob',\n    'trash',\n    'sucks',\n    'idiot',\n    'stupid',\n    'retard',\n    'kill yourself',\n    'kys',\n  ];\n\n  /**\n   * Moderate text content\n   */\n  async moderateText(userId: string, text: string): Promise<ModerationResult> {\n    try {\n      // Check for toxic keywords\n      const keywordCheck = this.checkToxicKeywords(text);\n\n      // Use OpenAI for advanced moderation\n      const aiModeration = await this.checkWithAI(text);\n\n      const isToxic = keywordCheck.isToxic || aiModeration.isToxic;\n      const toxicityScore = Math.max(keywordCheck.score, aiModeration.score);\n      const categories = [...new Set([...keywordCheck.categories, ...aiModeration.categories])];\n\n      // Determine action\n      let action: 'allow' | 'warn' | 'mute' | 'ban' = 'allow';\n      if (toxicityScore >= 0.8) {\n        action = 'ban';\n      } else if (toxicityScore >= 0.6) {\n        action = 'mute';\n      } else if (toxicityScore >= 0.4) {\n        action = 'warn';\n      }\n\n      // Log moderation action\n      if (isToxic) {\n        await this.logModerationAction(userId, text, action, toxicityScore, categories);\n      }\n\n      return {\n        text,\n        isToxic,\n        toxicityScore,\n        categories,\n        action,\n      };\n    } catch (error) {\n      logger.error({ message: 'Failed to moderate text', error });\n      return {\n        text,\n        isToxic: false,\n        toxicityScore: 0,\n        categories: [],\n        action: 'allow',\n      };\n    }\n  }\n\n  /**\n   * Check for toxic keywords\n   */\n  private checkToxicKeywords(text: string): { isToxic: boolean; score: number; categories: string[] } {\n    const lowerText = text.toLowerCase();\n    let score = 0;\n    const categories: string[] = [];\n\n    for (const keyword of this.toxicKeywords) {\n      if (lowerText.includes(keyword)) {\n        score += 0.2;\n        categories.push('profanity');\n      }\n    }\n\n    return {\n      isToxic: score > 0,\n      score: Math.min(score, 1),\n      categories,\n    };\n  }\n\n  /**\n   * Check with AI moderation\n   */\n  private async checkWithAI(text: string): Promise<{ isToxic: boolean; score: number; categories: string[] }> {\n    try {\n      const response = await openai.chat.completions.create({\n        model: 'gpt-4.1-mini',\n        messages: [\n          {\n            role: 'user',\n            content: `Analyze this text for toxicity, harassment, hate speech, and other harmful content. Return JSON:\n{\n  \"isToxic\": boolean,\n  \"score\": number (0-1),\n  \"categories\": string[]\n}\n\nText: \"${text}\"`,\n          },\n        ],\n        temperature: 0,\n        max_tokens: 100,\n      });\n\n      const content = response.choices[0]?.message?.content;\n      if (!content) {\n        return { isToxic: false, score: 0, categories: [] };\n      }\n\n      const result = JSON.parse(content);\n      return result;\n    } catch (error) {\n      logger.error({ message: 'Failed to check with AI moderation', error });\n      return { isToxic: false, score: 0, categories: [] };\n    }\n  }\n\n  /**\n   * Log moderation action\n   */\n  private async logModerationAction(\n    userId: string,\n    text: string,\n    action: string,\n    score: number,\n    categories: string[]\n  ): Promise<void> {\n    try {\n      await db.query(\n        `INSERT INTO moderation_logs (user_id, content, action, toxicity_score, categories)\n         VALUES ($1, $2, $3, $4, $5)`,\n        [userId, text, action, score, JSON.stringify(categories)]\n      );\n    } catch (error) {\n      logger.error({ message: 'Failed to log moderation action', error });\n    }\n  }\n\n  /**\n   * Get user moderation history\n   */\n  async getUserModerationHistory(userId: string): Promise<any[]> {\n    try {\n      const result = await db.query(\n        `SELECT * FROM moderation_logs WHERE user_id = $1 ORDER BY created_at DESC LIMIT 50`,\n        [userId]\n      );\n\n      return result.rows;\n    } catch (error) {\n      logger.error({ message: 'Failed to get moderation history', error });\n      return [];\n    }\n  }\n\n  /**\n   * Warn user\n   */\n  async warnUser(userId: string, reason: string): Promise<boolean> {\n    try {\n      await db.query(\n        `INSERT INTO user_warnings (user_id, reason)\n         VALUES ($1, $2)`,\n        [userId, reason]\n      );\n\n      logger.info(`User warned: ${userId} - ${reason}`);\n      return true;\n    } catch (error) {\n      logger.error({ message: 'Failed to warn user', error });\n      return false;\n    }\n  }\n\n  /**\n   * Mute user\n   */\n  async muteUser(userId: string, duration: number, reason: string): Promise<boolean> {\n    try {\n      const muteUntil = new Date(Date.now() + duration * 1000);\n\n      await db.query(\n        `INSERT INTO user_mutes (user_id, mute_until, reason)\n         VALUES ($1, $2, $3)\n         ON CONFLICT (user_id) DO UPDATE SET mute_until = $2, reason = $3`,\n        [userId, muteUntil, reason]\n      );\n\n      logger.info(`User muted: ${userId} - ${reason}`);\n      return true;\n    } catch (error) {\n      logger.error({ message: 'Failed to mute user', error });\n      return false;\n    }\n  }\n\n  /**\n   * Ban user\n   */\n  async banUser(userId: string, reason: string, permanent: boolean = false): Promise<boolean> {\n    try {\n      const banUntil = permanent ? null : new Date(Date.now() + 30 * 24 * 60 * 60 * 1000); // 30 days\n\n      await db.query(\n        `INSERT INTO user_bans (user_id, ban_until, reason, permanent)\n         VALUES ($1, $2, $3, $4)\n         ON CONFLICT (user_id) DO UPDATE SET ban_until = $2, reason = $3, permanent = $4`,\n        [userId, banUntil, reason, permanent]\n      );\n\n      logger.info(`User banned: ${userId} - ${reason}`);\n      return true;\n    } catch (error) {\n      logger.error({ message: 'Failed to ban user', error });\n      return false;\n    }\n  }\n\n  /**\n   * Check if user is muted\n   */\n  async isUserMuted(userId: string): Promise<boolean> {\n    try {\n      const result = await db.query(\n        'SELECT * FROM user_mutes WHERE user_id = $1 AND mute_until > NOW()',\n        [userId]\n      );\n\n      return result.rows.length > 0;\n    } catch (error) {\n      logger.error({ message: 'Failed to check if user is muted', error });\n      return false;\n    }\n  }\n\n  /**\n   * Check if user is banned\n   */\n  async isUserBanned(userId: string): Promise<boolean> {\n    try {\n      const result = await db.query(\n        `SELECT * FROM user_bans \n         WHERE user_id = $1 AND (permanent = true OR ban_until > NOW())`,\n        [userId]\n      );\n\n      return result.rows.length > 0;\n    } catch (error) {\n      logger.error({ message: 'Failed to check if user is banned', error });\n      return false;\n    }\n  }\n}\n\nexport default new ModerationService();\n
